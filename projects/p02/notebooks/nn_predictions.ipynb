{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 150, 150)          15600     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 150, 150)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 148, 150)          67650     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 148, 150)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 148, 150)          600       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 146, 150)          67650     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 146, 150)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 146, 150)          600       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 144, 150)          67650     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               22650     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1812      \n",
      "=================================================================\n",
      "Total params: 268,062\n",
      "Trainable params: 266,862\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import date\n",
    "from fastnumbers import isfloat, isint\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization, SpatialDropout1D, Conv1D, Dense, Dropout, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "rus_alphabet = ['а','б','в','г','д','е','ё','ж','з','и','й','к','л','м','н','о','п','р','с','т','у','ф','х','ц','ч','ш','щ','ъ','ы','ь','э','ю','я']\n",
    "\n",
    "def create_vocab_set():\n",
    "\n",
    "    alphabet = (rus_alphabet + list(string.ascii_lowercase) + list(string.digits) + list(string.punctuation) + [' ', '\\n'])\n",
    "    vocab_size = len(alphabet)\n",
    "    vocab = {}\n",
    "    for ix, t in enumerate(alphabet):\n",
    "        vocab[t] = ix+1\n",
    "    return vocab, vocab_size\n",
    "\n",
    "def text2sequence(text, vocab):\n",
    "    temp = []\n",
    "    for review in text:\n",
    "        temp.append([])\n",
    "        for i in review:\n",
    "            char = vocab.get(i,0)\n",
    "            if char != 0:\n",
    "                temp[-1].append(char)\n",
    "    return temp\n",
    "\n",
    "dir_train = '../data'\n",
    "\n",
    "mappings = {\n",
    "    'career': 0,\n",
    "    'theory_and_practice': 1,\n",
    "    'deep_learning': 2,\n",
    "    'lang_python': 3,\n",
    "    '_meetings': 4,\n",
    "    'kaggle_crackers': 5,\n",
    "    'big_data': 6,\n",
    "    'lang_r': 7,\n",
    "    'nlp': 8,\n",
    "    'welcome': 9,\n",
    "    'datasets': 10,\n",
    "    'bayesian': 11\n",
    "}\n",
    "\n",
    "\n",
    "# parameters initialization\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# initialize dictionary size and maximum sentence length\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "\n",
    "data = pd.read_csv(os.path.join(dir_train, 'train_set.csv'), usecols=range(1,11), parse_dates=['timestamp', 'thread_timestamp'])\n",
    "data = data[\n",
    "    data.channel.isin(['career', 'big_data', 'deep_learning', 'kaggle_crackers',\n",
    "           'lang_python',  'lang_r', 'nlp', 'theory_and_practice', 'welcome', 'bayesian', '_meetings', 'datasets']) &\n",
    "    data.main_msg\n",
    "]\n",
    "\n",
    "# data_train = data.\n",
    "date_before = date(2017, 4, 1)\n",
    "train = data[data['timestamp'] <= date_before]\n",
    "val = data[data['timestamp'] > date_before]\n",
    "\n",
    "train_data = train[['channel', 'text']].reset_index()[['channel', 'text']]\n",
    "train_data['channel'] = train_data.channel.map(mappings)\n",
    "train_data = train_data.sort_values('channel').reset_index()[['channel', 'text']]\n",
    "\n",
    "val_data = val[['channel', 'text']].reset_index()[['channel', 'text']]\n",
    "val_data['channel'] = val_data.channel.map(mappings)\n",
    "val_data = val_data.sort_values('channel').reset_index()[['channel', 'text']]\n",
    "\n",
    "train_data.text = train_data.text.astype(str)\\\n",
    "    .apply(lambda x: re.sub('(<\\S+>:?)|(\\s?:\\S+:\\s?)|(&gt;)|([\\w\\.]*@[\\w\\.]*)', ' ', x))\\\n",
    "    .apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "train_data = train_data[~train_data.text.apply(lambda x: isfloat(x) or isint(x) or len(x) < 20)]\n",
    "\n",
    "val_data.text = val_data.text.astype(str)\\\n",
    "    .apply(lambda x: re.sub('(<\\S+>:?)|(\\s?:\\S+:\\s?)|(&gt;)|([\\w\\.]*@[\\w\\.]*)', ' ', x))\\\n",
    "    .apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "val_data = val_data[~val_data.text.apply(lambda x: isfloat(x) or isint(x) or len(x) < 20)]\n",
    "\n",
    "train_text = train_data['text'].astype(str).apply(lambda x: x.lower())\n",
    "train_labels =  np.asarray(train_data['channel'], dtype='int8')\n",
    "\n",
    "val_text = val_data['text'].astype(str).apply(lambda x: x.lower())\n",
    "val_labels = np.asarray(val_data['channel'], dtype='int8')\n",
    "    \n",
    "vocab, vocab_size = create_vocab_set()\n",
    "\n",
    "X_train = text2sequence(train_text, vocab)\n",
    "X_val = text2sequence(val_text, vocab)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes=12)\n",
    "val_labels = to_categorical(val_labels, num_classes=12)\n",
    "\n",
    "model = load_model(\"../models/model_simple ohe lstm.hdf5\")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7733/7733 [==============================] - 54s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4525082519565289, 0.53860080176351388]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_predictions = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "import stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score, StratifiedKFold, ParameterGrid\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datetime import date\n",
    "import fastnumbers\n",
    "import re\n",
    "from fastnumbers import isfloat, isint\n",
    "\n",
    "train_text = train_data['text'].astype(str).apply(lambda x: x.lower())\n",
    "train_labels =  np.asarray(train_data['channel'], dtype='int8')\n",
    "\n",
    "val_text = val_data['text'].astype(str).apply(lambda x: x.lower())\n",
    "val_labels = np.asarray(val_data['channel'], dtype='int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# library for lemmatization and stop_words\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "# without tuning accuracy_score = 50.81%\n",
    "# accuracy_score = 55.33%\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(analyzer = 'char', max_features = 1000000, \n",
    "                                                       ngram_range = (1, 4))),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(),n_jobs=-1))])\n",
    "\n",
    "classifier.fit(train_text, train_labels)\n",
    "decision_boundary = classifier.decision_function(val_text)\n",
    "linear_predictions = np.exp(decision_boundary) / np.sum(np.exp(decision_boundary), axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ansamble_pred = np.argmax(linear_predictions * 0.63 + nn_predictions * (1 - 0.63), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    decision_boundary = classifier.decision_function([text])\n",
    "    linear_class = np.exp(decision_boundary) / np.sum(np.exp(decision_boundary), axis=1)[:, np.newaxis]\n",
    "    text_val = pad_sequences(text2sequence([text], vocab), maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "    nn_class = model.predict(text_val)\n",
    "    v = np.argmax(linear_class * 0.63 + nn_class * (1 - 0.63), axis = 1)[0]\n",
    "    for k in mappings:\n",
    "        if mappings[k] == v:\n",
    "            return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_text('где найти синтаксис дерево')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewsklyar/miniconda3/envs/py3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer = 'char', max_features = 1000000, ngram_range = (1, 4))\n",
    "train_matrix = vectorizer.fit_transform(train_text)\n",
    "val_matrix = vectorizer.fit_transform(val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "xgb_train = xgb.DMatrix(train_matrix, label=train_labels)\n",
    "xgb_val = xgb.DMatrix(val_matrix, label=val_labels)\n",
    "\n",
    "params = {\n",
    "    'eta': 0.1, \n",
    "    'seed': 42, \n",
    "    'subsample': 0.7, \n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'multi:softmax', \n",
    "    'max_depth': 7, \n",
    "    'min_child_weight': 1,\n",
    "    'num_class': 12,\n",
    "    'eval_metric': 'merror' \n",
    "}\n",
    "\n",
    "eval_matrix = [(xgb_val, 'xgb_val')]\n",
    "\n",
    "final_gb = xgb.train(params, xgb_train, num_boost_round = 1000, evals = eval_matrix, early_stopping_rounds=20,\n",
    "                    verbose_eval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_gb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c72faad5a04f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_gb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Predict using our testdmat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_gb' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_xgb = final_gb.predict(xgb_val) # Predict using our testdmat\n",
    "print(predictions_xgb.round().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train_matrix, label = train_labels)\n",
    "lgb_val = lgb.Dataset(val_matrix, label = val_labels, reference = lgb_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
