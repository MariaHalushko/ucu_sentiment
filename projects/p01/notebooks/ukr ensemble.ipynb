{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# data path initialization\n",
    "BASE_DIR = '../'\n",
    "TEXT_DATA_DIR = BASE_DIR + 'data/'\n",
    "TEXT_DATA_FILE = \"ukrainian_reviews_corpus.csv\"\n",
    "HEADER = True\n",
    "\n",
    "# parameters initialization\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "            Only computes a batch-wise average of recall.\n",
    "\n",
    "            Computes the recall, a metric for multi-label classification of\n",
    "            how many relevant items are selected.\n",
    "            \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "            Only computes a batch-wise average of precision.\n",
    "\n",
    "            Computes the precision, a metric for multi-label classification of\n",
    "            how many selected items are relevant.\n",
    "            \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # function for loading data\n",
    "    x = []\n",
    "    iy = -1\n",
    "    y = []\n",
    "    with open(os.path.join(TEXT_DATA_DIR, TEXT_DATA_FILE), \"r\", encoding=\"utf-8\") as f:\n",
    "        if HEADER:\n",
    "            _ = next(f)\n",
    "        for line in f:\n",
    "            if len(line) < 2 or line[1] != '|':\n",
    "                x[iy] = x[iy] + line.rstrip('\\n').replace(\"'\", \"\")\n",
    "            else:\n",
    "                temp_y, temp_x = line.rstrip(\"\\n\").split(\"|\", 1)\n",
    "                x.append(temp_x.replace(\"'\", \"\"))\n",
    "                y.append(temp_y)\n",
    "                iy += 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "data, labels = load_data()\n",
    "labels = np.asarray(labels, dtype='int8')\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "# spliting our original data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(data, np.asarray(labels, dtype='int8'),\n",
    "                                                                  test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED,\n",
    "                                                                  stratify=labels)\n",
    "\n",
    "# initialize dictionary size and maximum sentence length\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "ukr_alphabet = ['а', 'б', 'в', 'г', 'ґ', 'д', 'е', 'є', 'ж', 'з', 'і', 'ї', 'й',\n",
    "                'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'и',\n",
    "                'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ь', 'ю', 'я']\n",
    "\n",
    "\n",
    "def create_vocab_set():\n",
    "    alphabet = (ukr_alphabet + list(string.digits) +\n",
    "                list(string.punctuation) + list(string.whitespace))\n",
    "    vocab_size = len(alphabet)\n",
    "    vocab = {}\n",
    "    for ix, t in enumerate(alphabet):\n",
    "        vocab[t] = ix + 1\n",
    "    return vocab, vocab_size\n",
    "\n",
    "\n",
    "def text2sequence(text, vocab):\n",
    "    temp = []\n",
    "    for review in text:\n",
    "        temp.append([])\n",
    "        for i in review:\n",
    "            char = vocab.get(i, 0)\n",
    "            if char != 0:\n",
    "                temp[-1].append(char)\n",
    "    return temp\n",
    "\n",
    "\n",
    "vocab, vocab_size = create_vocab_set()\n",
    "\n",
    "X_train = text2sequence(data_train, vocab)\n",
    "X_val = text2sequence(data_val, vocab)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "\n",
    "import tensorflow as tf\n",
    "# ohe function\n",
    "def ohe(x, sz):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, MaxPooling1D, Dense, Conv1D, LSTM\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from keras import optimizers\n",
    "NAME = \"ohe_cnn_ukrainian\"\n",
    "# input initialization\n",
    "in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "# Lambda layer for ohe transformation\n",
    "embedded = Lambda(ohe, output_shape=lambda x: (x[0], x[1], vocab_size), arguments={\"sz\": vocab_size})(in_sentence)\n",
    "block = embedded\n",
    "# convolutions with MaxPooling\n",
    "for i in range(3):\n",
    "    block = Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\", trainable = True)(block)\n",
    "    if i == 0:\n",
    "        block = MaxPooling1D(pool_size=5)(block)\n",
    "# LSTM cell\n",
    "block = LSTM(128, dropout=0.1, recurrent_dropout=0.1, trainable = True)(block)\n",
    "block = Dense(100, activation='relu', trainable = True)(block)\n",
    "block = Dense(1, activation='sigmoid', trainable = True)(block)\n",
    "\n",
    "# callbacks initialization\n",
    "# automatic generation of learning curves\n",
    "callback_1 = TensorBoard(log_dir='../logs/logs_{}'.format(NAME), histogram_freq=0,\n",
    "                             write_graph=False, write_images=False)\n",
    "# stop training model if accuracy does not increase more than five epochs\n",
    "callback_2 = EarlyStopping(monitor='val_f1', min_delta=0, patience=5, verbose=0, mode='max')\n",
    "# best model saving\n",
    "callback_3 = ModelCheckpoint(\"models/model_{}.hdf5\".format(NAME), monitor='val_f1',\n",
    "                                 save_best_only=True, verbose=0, mode='max')\n",
    "\n",
    "\n",
    "# initialize model\n",
    "model = Model(inputs=in_sentence, outputs=block)\n",
    "\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('../models/model_ohe_cnn_ukrainian.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(model.predict(X_val)[:,0] > 0.5, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88937093275488066"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_val, np.array(model.predict(X_val)[:,0] > 0.5, int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../'\n",
    "TEXT_DATA_DIR = BASE_DIR + 'data/'\n",
    "TEXT_DATA_FILE = \"ukrainian_reviews_corpus.csv\"\n",
    "HEADER = True\n",
    "\n",
    "# parameters initialization\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "            Only computes a batch-wise average of recall.\n",
    "\n",
    "            Computes the recall, a metric for multi-label classification of\n",
    "            how many relevant items are selected.\n",
    "            \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "            Only computes a batch-wise average of precision.\n",
    "\n",
    "            Computes the precision, a metric for multi-label classification of\n",
    "            how many selected items are relevant.\n",
    "            \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # function for loading data\n",
    "    x = []\n",
    "    iy = -1\n",
    "    y = []\n",
    "    with open(os.path.join(TEXT_DATA_DIR, TEXT_DATA_FILE), \"r\", encoding=\"utf-8\") as f:\n",
    "        if HEADER:\n",
    "            _ = next(f)\n",
    "        for line in f:\n",
    "            line = line.replace('и', 'ы')\n",
    "            line = line.replace('і', 'и')\n",
    "            line = line.replace('є', 'е')\n",
    "            line = line.replace('ї', 'и')\n",
    "            if len(line) < 2 or line[1] != '|':\n",
    "                x[iy] = x[iy] + line.rstrip('\\n').replace(\"'\", \"\")\n",
    "            else:\n",
    "                temp_y, temp_x = line.rstrip(\"\\n\").split(\"|\", 1)\n",
    "                x.append(temp_x.replace(\"'\", \"\"))\n",
    "                y.append(temp_y)\n",
    "                iy += 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "data, labels = load_data()\n",
    "labels = np.asarray(labels, dtype='int8')\n",
    "\n",
    "# spliting our original data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(data, np.asarray(labels, dtype='int8'),\n",
    "                                                                  test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED,\n",
    "                                                                  stratify=labels)\\\n",
    "\n",
    "# initialize dictionary size and maximum sentence length\n",
    "MAX_NB_WORDS = 81\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "rus_alphabet = ['а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у',\n",
    "                'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я']\n",
    "\n",
    "alphabet = (list(rus_alphabet) + list(string.digits) + list(string.punctuation) + list(string.whitespace))\n",
    "vocab_size = len(alphabet)\n",
    "\n",
    "\n",
    "def create_vocab_set():\n",
    "    alphabet = (list(rus_alphabet) + list(string.digits) +\n",
    "                list(string.punctuation) + list(string.whitespace))\n",
    "    vocab_size = len(alphabet)\n",
    "    vocab = {}\n",
    "    for ix, t in enumerate(alphabet):\n",
    "        vocab[t] = ix + 1\n",
    "    return vocab, vocab_size\n",
    "\n",
    "\n",
    "def text2sequence(text, vocab):\n",
    "    temp = []\n",
    "    for review in text:\n",
    "        temp.append([])\n",
    "        for i in review:\n",
    "            char = vocab.get(i, 0)\n",
    "            if char != 0:\n",
    "                temp[-1].append(char)\n",
    "    return temp\n",
    "\n",
    "\n",
    "vocab, vocab_size = create_vocab_set()\n",
    "\n",
    "X_train = text2sequence(data_train, vocab)\n",
    "X_val = text2sequence(data_val, vocab)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "\n",
    "import tensorflow as tf\n",
    "# ohe function\n",
    "def ohe(x, sz):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, MaxPooling1D, Dense, Conv1D, LSTM\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from keras import optimizers\n",
    "NAME = \"transfer_learning_from_russian_to_ukrainian\"\n",
    "# input initialization\n",
    "in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "# Lambda layer for ohe transformation\n",
    "embedded = Lambda(ohe, output_shape=lambda x: (x[0], x[1], vocab_size), arguments={\"sz\": vocab_size})(in_sentence)\n",
    "block = embedded\n",
    "# convolutions with MaxPooling\n",
    "for i in range(3):\n",
    "    block = Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\", trainable = True)(block)\n",
    "    if i == 0:\n",
    "        block = MaxPooling1D(pool_size=5)(block)\n",
    "# LSTM cell\n",
    "block = LSTM(128, dropout=0.1, recurrent_dropout=0.1, trainable = True)(block)\n",
    "block = Dense(100, activation='relu', trainable = True)(block)\n",
    "block = Dense(1, activation='sigmoid', trainable = True)(block)\n",
    "\n",
    "# callbacks initialization\n",
    "# automatic generation of learning curves\n",
    "callback_1 = TensorBoard(log_dir='../logs/logs_{}'.format(NAME), histogram_freq=0,\n",
    "                             write_graph=False, write_images=False)\n",
    "# stop training model if accuracy does not increase more than five epochs\n",
    "callback_2 = EarlyStopping(monitor='val_f1', min_delta=0, patience=5, verbose=0, mode='max')\n",
    "# best model saving\n",
    "callback_3 = ModelCheckpoint(\"models/model_{}.hdf5\".format(NAME), monitor='val_f1',\n",
    "                                 save_best_only=True, verbose=0, mode='max')\n",
    "\n",
    "\n",
    "def update_func(i):\n",
    "    base = 0.01\n",
    "    lr = base / (10**(i // 4 + 1))\n",
    "    if lr < 0.00001:\n",
    "        return 0.00001\n",
    "    return lr\n",
    "\n",
    "callback_4 = LearningRateScheduler(update_func)\n",
    "# initialize model\n",
    "model_transfer = Model(inputs=in_sentence, outputs=block)\n",
    "model_transfer.load_weights('../models/model_transfer_learning_from_russian_to_ukrainian.hdf5')\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model_transfer.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9137931034482758"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_val, np.array(model_transfer.predict(X_val)[:,0] > 0.5, int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "model_linear = pickle.load(open(\"../models/spanish_linear_model.pkl\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 : 0.850833333333\n",
      "0.010101010101 : 0.850478967097\n",
      "0.020202020202 : 0.85012489592\n",
      "0.030303030303 : 0.851435705368\n",
      "0.040404040404 : 0.851435705368\n",
      "0.0505050505051 : 0.851435705368\n",
      "0.0606060606061 : 0.850957535387\n",
      "0.0707070707071 : 0.850478967097\n",
      "0.0808080808081 : 0.851435705368\n",
      "0.0909090909091 : 0.851435705368\n",
      "0.10101010101 : 0.851081530782\n",
      "0.111111111111 : 0.851913477537\n",
      "0.121212121212 : 0.851913477537\n",
      "0.131313131313 : 0.852036575229\n",
      "0.141414141414 : 0.853344412131\n",
      "0.151515151515 : 0.854175321978\n",
      "0.161616161616 : 0.853699085619\n",
      "0.171717171717 : 0.854175321978\n",
      "0.181818181818 : 0.85536159601\n",
      "0.191919191919 : 0.854651162791\n",
      "0.20202020202 : 0.855006231824\n",
      "0.212121212121 : 0.85536159601\n",
      "0.222222222222 : 0.85536159601\n",
      "0.232323232323 : 0.854885654886\n",
      "0.242424242424 : 0.854530340815\n",
      "0.252525252525 : 0.853699085619\n",
      "0.262626262626 : 0.854530340815\n",
      "0.272727272727 : 0.854409317804\n",
      "0.282828282828 : 0.855006231824\n",
      "0.292929292929 : 0.85595682856\n",
      "0.30303030303 : 0.856787048568\n",
      "0.313131313131 : 0.856668051516\n",
      "0.323232323232 : 0.857498961363\n",
      "0.333333333333 : 0.856668051516\n",
      "0.343434343434 : 0.857498961363\n",
      "0.353535353535 : 0.858091286307\n",
      "0.363636363636 : 0.857024450891\n",
      "0.373737373737 : 0.859859446052\n",
      "0.383838383838 : 0.859859446052\n",
      "0.393939393939 : 0.859620148637\n",
      "0.40404040404 : 0.861271676301\n",
      "0.414141414141 : 0.861855670103\n",
      "0.424242424242 : 0.861373920197\n",
      "0.434343434343 : 0.861842105263\n",
      "0.444444444444 : 0.861715223636\n",
      "0.454545454545 : 0.862182116489\n",
      "0.464646464646 : 0.861715223636\n",
      "0.474747474747 : 0.8605414274\n",
      "0.484848484848 : 0.860188601886\n",
      "0.494949494949 : 0.857961522718\n",
      "0.505050505051 : 0.854693877551\n",
      "0.515151515152 : 0.852124183007\n",
      "0.525252525253 : 0.849694501018\n",
      "0.535353535354 : 0.850060950833\n",
      "0.545454545455 : 0.849837662338\n",
      "0.555555555556 : 0.85170178282\n",
      "0.565656565657 : 0.850101010101\n",
      "0.575757575758 : 0.849757673667\n",
      "0.585858585859 : 0.849292929293\n",
      "0.59595959596 : 0.849757673667\n",
      "0.606060606061 : 0.847922549415\n",
      "0.616161616162 : 0.848045143087\n",
      "0.626262626263 : 0.847826086957\n",
      "0.636363636364 : 0.846803377563\n",
      "0.646464646465 : 0.844515869827\n",
      "0.656565656566 : 0.844176706827\n",
      "0.666666666667 : 0.843034925733\n",
      "0.676767676768 : 0.842696629213\n",
      "0.686868686869 : 0.842232035327\n",
      "0.69696969697 : 0.842358604091\n",
      "0.707070707071 : 0.841218925421\n",
      "0.717171717172 : 0.840544871795\n",
      "0.727272727273 : 0.840080160321\n",
      "0.737373737374 : 0.838942307692\n",
      "0.747474747475 : 0.838529176659\n",
      "0.757575757576 : 0.836131095124\n",
      "0.767676767677 : 0.836261980831\n",
      "0.777777777778 : 0.836261980831\n",
      "0.787878787879 : 0.836523125997\n",
      "0.79797979798 : 0.835725677831\n",
      "0.808080808081 : 0.833930704898\n",
      "0.818181818182 : 0.831942789035\n",
      "0.828282828283 : 0.831478537361\n",
      "0.838383838384 : 0.831013916501\n",
      "0.848484848485 : 0.829888712242\n",
      "0.858585858586 : 0.829423459245\n",
      "0.868686868687 : 0.829365079365\n",
      "0.878787878788 : 0.827449424831\n",
      "0.888888888889 : 0.8264659271\n",
      "0.89898989899 : 0.826328310864\n",
      "0.909090909091 : 0.826138613861\n",
      "0.919191919192 : 0.824367088608\n",
      "0.929292929293 : 0.82320441989\n",
      "0.939393939394 : 0.822879684418\n",
      "0.949494949495 : 0.822415153907\n",
      "0.959595959596 : 0.821118991332\n",
      "0.969696969697 : 0.820654316121\n",
      "0.979797979798 : 0.820654316121\n",
      "0.989898989899 : 0.820007877117\n",
      "1.0 : 0.819220165419\n"
     ]
    }
   ],
   "source": [
    "for i in np.linspace(0,1,100):\n",
    "    print(i, \":\", f1_score(labels_val,np.array((preds_nn_val*(i) + preds_lr_val*(1-i))[:,1] > 0.5, 'int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 : 0.850833333333\n",
      "0.010101010101 : 0.851666666667\n",
      "0.020202020202 : 0.85154295246\n",
      "0.030303030303 : 0.852977925864\n",
      "0.040404040404 : 0.853333333333\n",
      "0.0505050505051 : 0.852732582395\n",
      "0.0606060606061 : 0.853566958698\n",
      "0.0707070707071 : 0.853566958698\n",
      "0.0808080808081 : 0.85260960334\n",
      "0.0909090909091 : 0.853923205342\n",
      "0.10101010101 : 0.854879065888\n",
      "0.111111111111 : 0.855\n",
      "0.121212121212 : 0.855356398499\n",
      "0.131313131313 : 0.855833333333\n",
      "0.141414141414 : 0.856666666667\n",
      "0.151515151515 : 0.856427378965\n",
      "0.161616161616 : 0.856784968685\n",
      "0.171717171717 : 0.856784968685\n",
      "0.181818181818 : 0.855949895616\n",
      "0.191919191919 : 0.855949895616\n",
      "0.20202020202 : 0.856307435255\n",
      "0.212121212121 : 0.855708908407\n",
      "0.222222222222 : 0.85690376569\n",
      "0.232323232323 : 0.857859531773\n",
      "0.242424242424 : 0.857859531773\n",
      "0.252525252525 : 0.857381848599\n",
      "0.262626262626 : 0.857740585774\n",
      "0.272727272727 : 0.856783919598\n",
      "0.282828282828 : 0.856783919598\n",
      "0.292929292929 : 0.856783919598\n",
      "0.30303030303 : 0.857621440536\n",
      "0.313131313131 : 0.857142857143\n",
      "0.323232323232 : 0.857023060797\n",
      "0.333333333333 : 0.855467113532\n",
      "0.343434343434 : 0.856783919598\n",
      "0.353535353535 : 0.856783919598\n",
      "0.363636363636 : 0.856783919598\n",
      "0.373737373737 : 0.857501044714\n",
      "0.383838383838 : 0.855833333333\n",
      "0.393939393939 : 0.855006231824\n",
      "0.40404040404 : 0.854296388543\n",
      "0.414141414141 : 0.853587722937\n",
      "0.424242424242 : 0.856550580431\n",
      "0.434343434343 : 0.855366763365\n",
      "0.444444444444 : 0.855132450331\n",
      "0.454545454545 : 0.852770885029\n",
      "0.464646464646 : 0.852892561983\n",
      "0.474747474747 : 0.852540272615\n",
      "0.484848484848 : 0.851485148515\n",
      "0.494949494949 : 0.851485148515\n",
      "0.505050505051 : 0.852202552491\n",
      "0.515151515152 : 0.851851851852\n",
      "0.525252525253 : 0.851151315789\n",
      "0.535353535354 : 0.849506578947\n",
      "0.545454545455 : 0.849157418824\n",
      "0.555555555556 : 0.84858432499\n",
      "0.565656565657 : 0.846500204666\n",
      "0.575757575758 : 0.845208845209\n",
      "0.585858585859 : 0.844735764031\n",
      "0.59595959596 : 0.843698854337\n",
      "0.606060606061 : 0.841717791411\n",
      "0.616161616162 : 0.84\n",
      "0.626262626263 : 0.839788014676\n",
      "0.636363636364 : 0.838420838421\n",
      "0.646464646465 : 0.838079739626\n",
      "0.656565656566 : 0.836452400325\n",
      "0.666666666667 : 0.835638730675\n",
      "0.676767676768 : 0.836452400325\n",
      "0.686868686869 : 0.835905767669\n",
      "0.69696969697 : 0.83556638246\n",
      "0.707070707071 : 0.83556638246\n",
      "0.717171717172 : 0.83569979716\n",
      "0.727272727273 : 0.834888438134\n",
      "0.737373737374 : 0.834888438134\n",
      "0.747474747475 : 0.834683954619\n",
      "0.757575757576 : 0.833063209076\n",
      "0.767676767677 : 0.832252836305\n",
      "0.777777777778 : 0.832252836305\n",
      "0.787878787879 : 0.831915755367\n",
      "0.79797979798 : 0.831915755367\n",
      "0.808080808081 : 0.831578947368\n",
      "0.818181818182 : 0.831578947368\n",
      "0.828282828283 : 0.830769230769\n",
      "0.838383838384 : 0.830433023068\n",
      "0.848484848485 : 0.830906148867\n",
      "0.858585858586 : 0.829623634156\n",
      "0.868686868687 : 0.830097087379\n",
      "0.878787878788 : 0.830097087379\n",
      "0.888888888889 : 0.829090909091\n",
      "0.89898989899 : 0.828756058158\n",
      "0.909090909091 : 0.828282828283\n",
      "0.919191919192 : 0.827614049253\n",
      "0.929292929293 : 0.827140549273\n",
      "0.939393939394 : 0.826472962066\n",
      "0.949494949495 : 0.826139572408\n",
      "0.959595959596 : 0.825665859564\n",
      "0.969696969697 : 0.825806451613\n",
      "0.979797979798 : 0.826612903226\n",
      "0.989898989899 : 0.825473599355\n",
      "1.0 : 0.825281803543\n"
     ]
    }
   ],
   "source": [
    "for i in np.linspace(0,1,100):\n",
    "    print(i, \":\", f1_score(labels_val,np.array((preds_tr_nn_val*(i) + preds_lr_val*(1-i))[:,1] > 0.5, 'int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.85948058,  0.87535394],\n",
       "       [ 0.85948058,  1.        ,  0.87642146],\n",
       "       [ 0.87535394,  0.87642146,  1.        ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([preds_nn_val[:,0], preds_tr_nn_val[:,0], preds_lr_val[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99341279],\n",
       "       [ 0.9970293 ],\n",
       "       [ 0.90376759],\n",
       "       ..., \n",
       "       [ 0.0013071 ],\n",
       "       [ 0.95976818],\n",
       "       [ 0.04299338]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_nn_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.850833333333\n",
      "0.0 0.0526315789474 0.852732582395\n",
      "0.0 0.105263157895 0.854522717799\n",
      "0.0 0.157894736842 0.856784968685\n",
      "0.0 0.210526315789 0.85618729097\n",
      "0.0 0.263157894737 0.857740585774\n",
      "0.0 0.315789473684 0.85666387259\n",
      "0.0 0.368421052632 0.857859531773\n",
      "0.0 0.421052631579 0.856195607128\n",
      "0.0 0.473684210526 0.852892561983\n",
      "0.0 0.526315789474 0.850801479655\n",
      "0.0 0.578947368421 0.845681539091\n",
      "0.0 0.631578947368 0.838420838421\n",
      "0.0 0.684210526316 0.836245428688\n",
      "0.0 0.736842105263 0.834888438134\n",
      "0.0 0.789473684211 0.832388663968\n",
      "0.0 0.842105263158 0.830906148867\n",
      "0.0 0.894736842105 0.829090909091\n",
      "0.0 0.947368421053 0.826139572408\n",
      "0.0526315789474 0.0 0.851435705368\n",
      "0.0526315789474 0.0526315789474 0.854288093256\n",
      "0.0526315789474 0.105263157895 0.856786011657\n",
      "0.0526315789474 0.157894736842 0.857262103506\n",
      "0.0526315789474 0.210526315789 0.857501044714\n",
      "0.0526315789474 0.263157894737 0.860484544695\n",
      "0.0526315789474 0.315789473684 0.859531772575\n",
      "0.0526315789474 0.368421052632 0.85643153527\n",
      "0.0526315789474 0.421052631579 0.855721393035\n",
      "0.0526315789474 0.473684210526 0.85043263288\n",
      "0.0526315789474 0.526315789474 0.848932676519\n",
      "0.0526315789474 0.578947368421 0.841932841933\n",
      "0.0526315789474 0.631578947368 0.838552257015\n",
      "0.0526315789474 0.684210526316 0.835227272727\n",
      "0.0526315789474 0.736842105263 0.832927818329\n",
      "0.0526315789474 0.789473684211 0.831715210356\n",
      "0.0526315789474 0.842105263158 0.83212267958\n",
      "0.0526315789474 0.894736842105 0.827614049253\n",
      "0.105263157895 0.0 0.851790174854\n",
      "0.105263157895 0.0526315789474 0.854522717799\n",
      "0.105263157895 0.105263157895 0.856904463913\n",
      "0.105263157895 0.157894736842 0.857620041754\n",
      "0.105263157895 0.210526315789 0.858813700919\n",
      "0.105263157895 0.263157894737 0.861192163401\n",
      "0.105263157895 0.315789473684 0.860465116279\n",
      "0.105263157895 0.368421052632 0.858208955224\n",
      "0.105263157895 0.421052631579 0.854545454545\n",
      "0.105263157895 0.473684210526 0.850574712644\n",
      "0.105263157895 0.526315789474 0.846846846847\n",
      "0.105263157895 0.578947368421 0.842448979592\n",
      "0.105263157895 0.631578947368 0.838211382114\n",
      "0.105263157895 0.684210526316 0.834959349593\n",
      "0.105263157895 0.736842105263 0.833063209076\n",
      "0.105263157895 0.789473684211 0.830980233965\n",
      "0.105263157895 0.842105263158 0.828364222401\n",
      "0.105263157895 0.894736842105 0.826139572408\n",
      "0.157894736842 0.0 0.853699085619\n",
      "0.157894736842 0.0526315789474 0.855833333333\n",
      "0.157894736842 0.105263157895 0.858813700919\n",
      "0.157894736842 0.157894736842 0.86\n",
      "0.157894736842 0.210526315789 0.861180382377\n",
      "0.157894736842 0.263157894737 0.863542098714\n",
      "0.157894736842 0.315789473684 0.862712567399\n",
      "0.157894736842 0.368421052632 0.858205870194\n",
      "0.157894736842 0.421052631579 0.855144032922\n",
      "0.157894736842 0.473684210526 0.852459016393\n",
      "0.157894736842 0.526315789474 0.846530612245\n",
      "0.157894736842 0.578947368421 0.842490842491\n",
      "0.157894736842 0.631578947368 0.837398373984\n",
      "0.157894736842 0.684210526316 0.833265720081\n",
      "0.157894736842 0.736842105263 0.831378892034\n",
      "0.157894736842 0.789473684211 0.829838709677\n",
      "0.210526315789 0.0 0.855006231824\n",
      "0.210526315789 0.0526315789474 0.856429463171\n",
      "0.210526315789 0.105263157895 0.858926342072\n",
      "0.210526315789 0.157894736842 0.862956810631\n",
      "0.210526315789 0.210526315789 0.863542098714\n",
      "0.210526315789 0.263157894737 0.86636326024\n",
      "0.210526315789 0.315789473684 0.864441697569\n",
      "0.210526315789 0.368421052632 0.858552631579\n",
      "0.210526315789 0.421052631579 0.854560394412\n",
      "0.210526315789 0.473684210526 0.850593532542\n",
      "0.210526315789 0.526315789474 0.845024469821\n",
      "0.210526315789 0.578947368421 0.841805612037\n",
      "0.210526315789 0.631578947368 0.837662337662\n",
      "0.210526315789 0.684210526316 0.834211593028\n",
      "0.210526315789 0.736842105263 0.830843762616\n",
      "0.263157894737 0.0 0.854530340815\n",
      "0.263157894737 0.0526315789474 0.857498961363\n",
      "0.263157894737 0.105263157895 0.861410788382\n",
      "0.263157894737 0.157894736842 0.862225899876\n",
      "0.263157894737 0.210526315789 0.86435070306\n",
      "0.263157894737 0.263157894737 0.864085667216\n",
      "0.263157894737 0.315789473684 0.86288998358\n",
      "0.263157894737 0.368421052632 0.856674856675\n",
      "0.263157894737 0.421052631579 0.852230863692\n",
      "0.263157894737 0.473684210526 0.849673202614\n",
      "0.263157894737 0.526315789474 0.842961757526\n",
      "0.263157894737 0.578947368421 0.84103811841\n",
      "0.263157894737 0.631578947368 0.838526912181\n",
      "0.263157894737 0.684210526316 0.836231298019\n",
      "0.315789473684 0.0 0.856312292359\n",
      "0.315789473684 0.0526315789474 0.858091286307\n",
      "0.315789473684 0.105263157895 0.860801321768\n",
      "0.315789473684 0.157894736842 0.862225899876\n",
      "0.315789473684 0.210526315789 0.86361763494\n",
      "0.315789473684 0.263157894737 0.863244353183\n",
      "0.315789473684 0.315789473684 0.858780188293\n",
      "0.315789473684 0.368421052632 0.856326530612\n",
      "0.315789473684 0.421052631579 0.852365415987\n",
      "0.315789473684 0.473684210526 0.849694501018\n",
      "0.315789473684 0.526315789474 0.844715447154\n",
      "0.315789473684 0.578947368421 0.841295546559\n",
      "0.315789473684 0.631578947368 0.838787878788\n",
      "0.315789473684 0.684210526316 0.835351089588\n",
      "0.368421052632 0.0 0.857971014493\n",
      "0.368421052632 0.0526315789474 0.860445912469\n",
      "0.368421052632 0.105263157895 0.862567065621\n",
      "0.368421052632 0.157894736842 0.861728395062\n",
      "0.368421052632 0.210526315789 0.861361771944\n",
      "0.368421052632 0.263157894737 0.861224489796\n",
      "0.368421052632 0.315789473684 0.856445709638\n",
      "0.368421052632 0.368421052632 0.857954545455\n",
      "0.368421052632 0.421052631579 0.856331168831\n",
      "0.368421052632 0.473684210526 0.849348534202\n",
      "0.368421052632 0.526315789474 0.844282238443\n",
      "0.368421052632 0.578947368421 0.839466235342\n",
      "0.421052631579 0.0 0.860905349794\n",
      "0.421052631579 0.0526315789474 0.861019736842\n",
      "0.421052631579 0.105263157895 0.860426929392\n",
      "0.421052631579 0.157894736842 0.860065466448\n",
      "0.421052631579 0.210526315789 0.859477124183\n",
      "0.421052631579 0.263157894737 0.857839155158\n",
      "0.421052631579 0.315789473684 0.854707792208\n",
      "0.421052631579 0.368421052632 0.854251012146\n",
      "0.421052631579 0.421052631579 0.852738336714\n",
      "0.421052631579 0.473684210526 0.846715328467\n",
      "0.421052631579 0.526315789474 0.843636363636\n",
      "0.473684210526 0.0 0.8605414274\n",
      "0.473684210526 0.0526315789474 0.85866448177\n",
      "0.473684210526 0.105263157895 0.859591836735\n",
      "0.473684210526 0.157894736842 0.859120521173\n",
      "0.473684210526 0.210526315789 0.858417849899\n",
      "0.473684210526 0.263157894737 0.853905301497\n",
      "0.473684210526 0.315789473684 0.853451756157\n",
      "0.473684210526 0.368421052632 0.854023453296\n",
      "0.473684210526 0.421052631579 0.849979781642\n",
      "0.473684210526 0.473684210526 0.846620801295\n",
      "0.526315789474 0.0 0.85016286645\n",
      "0.526315789474 0.0526315789474 0.85168630638\n",
      "0.526315789474 0.105263157895 0.852047020673\n",
      "0.526315789474 0.157894736842 0.853095912586\n",
      "0.526315789474 0.210526315789 0.851612903226\n",
      "0.526315789474 0.263157894737 0.849657396211\n",
      "0.526315789474 0.315789473684 0.848753016895\n",
      "0.526315789474 0.368421052632 0.847580645161\n",
      "0.526315789474 0.421052631579 0.846803377563\n",
      "0.578947368421 0.0 0.849292929293\n",
      "0.578947368421 0.0526315789474 0.850463522773\n",
      "0.578947368421 0.105263157895 0.851971037812\n",
      "0.578947368421 0.157894736842 0.847852268165\n",
      "0.578947368421 0.210526315789 0.84707766213\n",
      "0.578947368421 0.263157894737 0.846645367412\n",
      "0.578947368421 0.315789473684 0.844213055667\n",
      "0.578947368421 0.368421052632 0.844586496205\n",
      "0.631578947368 0.0 0.847144006436\n",
      "0.631578947368 0.0526315789474 0.847852268165\n",
      "0.631578947368 0.105263157895 0.846615939127\n",
      "0.631578947368 0.157894736842 0.84606157537\n",
      "0.631578947368 0.210526315789 0.843912175649\n",
      "0.631578947368 0.263157894737 0.8418956591\n",
      "0.631578947368 0.315789473684 0.841769629334\n",
      "0.684210526316 0.0 0.842232035327\n",
      "0.684210526316 0.0526315789474 0.843074459568\n",
      "0.684210526316 0.105263157895 0.841853035144\n",
      "0.684210526316 0.157894736842 0.840047865975\n",
      "0.684210526316 0.210526315789 0.838581108011\n",
      "0.684210526316 0.263157894737 0.837708830549\n",
      "0.684210526316 0.315789473684 0.83645045762\n",
      "0.736842105263 0.0 0.838942307692\n",
      "0.736842105263 0.0526315789474 0.839664134346\n",
      "0.736842105263 0.105263157895 0.83698684735\n",
      "0.736842105263 0.157894736842 0.83645045762\n",
      "0.736842105263 0.210526315789 0.834194831014\n",
      "0.789473684211 0.0 0.836856800957\n",
      "0.789473684211 0.0526315789474 0.83724631914\n",
      "0.789473684211 0.105263157895 0.835453100159\n",
      "0.789473684211 0.157894736842 0.832209440698\n",
      "0.842105263158 0.0 0.830683624801\n",
      "0.842105263158 0.0526315789474 0.830023828435\n",
      "0.842105263158 0.105263157895 0.829635499208\n",
      "0.894736842105 0.0 0.826328310864\n",
      "0.894736842105 0.0526315789474 0.826603325416\n",
      "0.894736842105 0.105263157895 0.825434439179\n",
      "0.947368421053 0.0 0.822415153907\n"
     ]
    }
   ],
   "source": [
    "for i in np.linspace(0, 1, 20):\n",
    "    for j in np.linspace(0, 1, 20):\n",
    "        if i + j < 1:\n",
    "            print(i, j, f1_score(labels_val,np.array((preds_tr_nn_val*(j) + preds_nn_val*(i) + preds_lr_val*(1-i-j))[:,1] > 0.5, 'int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
