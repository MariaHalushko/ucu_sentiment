{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "# data path initialization\n",
    "BASE_DIR = '../'\n",
    "TEXT_DATA_DIR = BASE_DIR + 'data/'\n",
    "TEXT_DATA_FILE = \"ukrainian_reviews_corpus.csv\"\n",
    "HEADER = True\n",
    "\n",
    "# parameters initialization\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "            \n",
    "            Only computes a batch-wise average of recall.\n",
    "            \n",
    "            Computes the recall, a metric for multi-label classification of\n",
    "            how many relevant items are selected.\n",
    "            \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    \n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "            \n",
    "            Only computes a batch-wise average of precision.\n",
    "            \n",
    "            Computes the precision, a metric for multi-label classification of\n",
    "            how many selected items are relevant.\n",
    "            \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # function for loading data\n",
    "    x = []\n",
    "    iy = -1\n",
    "    y = []\n",
    "    with open(os.path.join(TEXT_DATA_DIR, TEXT_DATA_FILE), \"r\", encoding=\"utf-8\") as f:\n",
    "        if HEADER:\n",
    "            _ = next(f)\n",
    "        for line in f:\n",
    "            line = line.replace('и','ы')\n",
    "            line = line.replace('і','и')\n",
    "            line = line.replace('є','е')\n",
    "            line = line.replace('ї','и')\n",
    "            if len(line) < 2 or line[1] != '|':\n",
    "                x[iy] = x[iy] + line.rstrip('\\n').replace(\"'\", \"\")\n",
    "            else:\n",
    "                temp_y, temp_x = line.rstrip(\"\\n\").split(\"|\", 1)\n",
    "                x.append(temp_x.replace(\"'\", \"\"))\n",
    "                y.append(temp_y)\n",
    "                iy += 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "data, labels = load_data()\n",
    "labels = np.asarray(labels, dtype='int8')\n",
    "\n",
    "# spliting our original data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val =     train_test_split(data, np.asarray(labels, dtype='int8'),\n",
    "                     test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=labels)\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize dictionary size and maximum sentence length\n",
    "MAX_NB_WORDS = 81\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "\n",
    "\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "rus_alphabet = ['а','б','в','г','д','е','ё','ж','з','и','й','к','л','м','н','о','п','р','с','т','у','ф','х','ц','ч','ш','щ','ъ','ы','ь','э','ю','я']\n",
    "\n",
    "\n",
    "\n",
    "alphabet = (list(rus_alphabet) + list(string.digits) + list(string.punctuation) + list(string.whitespace))\n",
    "vocab_size = len(alphabet)\n",
    "\n",
    "\n",
    "\n",
    "def create_vocab_set():\n",
    "    \n",
    "    alphabet = (list(rus_alphabet) + list(string.digits) + \n",
    "                              list(string.punctuation) + list(string.whitespace))\n",
    "    vocab_size = len(alphabet)\n",
    "    vocab = {}\n",
    "    for ix, t in enumerate(alphabet):\n",
    "        vocab[t] = ix+1\n",
    "    return vocab, vocab_size\n",
    "\n",
    "def text2sequence(text, vocab):\n",
    "    temp = []\n",
    "    for review in text:\n",
    "                     temp.append([])\n",
    "                     for i in review:\n",
    "                         char = vocab.get(i,0)\n",
    "                         if char != 0:\n",
    "                            temp[-1].append(char)\n",
    "    return temp\n",
    "\n",
    "\n",
    "\n",
    "vocab, vocab_size = create_vocab_set()\n",
    "\n",
    "X_train = text2sequence(data_train, vocab)\n",
    "X_val = text2sequence(data_val, vocab)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# ohe function\n",
    "def ohe(x, sz):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, MaxPooling1D, Dense, Conv1D, LSTM\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 400, 81)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 397, 100)          32500     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 79, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 76, 100)           40100     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 73, 100)           40100     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 242,949\n",
      "Trainable params: 242,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 22105 samples, validate on 2457 samples\n",
      "Epoch 1/1000\n",
      "22105/22105 [==============================] - 190s - loss: 0.2908 - f1: 0.8379 - val_loss: 0.2726 - val_f1: 0.8480\n",
      "Epoch 2/1000\n",
      "22105/22105 [==============================] - 176s - loss: 0.2423 - f1: 0.8723 - val_loss: 0.2539 - val_f1: 0.8614\n",
      "Epoch 3/1000\n",
      "22105/22105 [==============================] - 171s - loss: 0.2244 - f1: 0.8814 - val_loss: 0.2494 - val_f1: 0.8700\n",
      "Epoch 4/1000\n",
      "22105/22105 [==============================] - 160s - loss: 0.2128 - f1: 0.8915 - val_loss: 0.2370 - val_f1: 0.8820\n",
      "Epoch 5/1000\n",
      "22105/22105 [==============================] - 162s - loss: 0.2035 - f1: 0.8979 - val_loss: 0.2338 - val_f1: 0.8837\n",
      "Epoch 6/1000\n",
      "22105/22105 [==============================] - 161s - loss: 0.1945 - f1: 0.9029 - val_loss: 0.2321 - val_f1: 0.8811\n",
      "Epoch 7/1000\n",
      "22105/22105 [==============================] - 161s - loss: 0.1878 - f1: 0.9092 - val_loss: 0.2294 - val_f1: 0.8797\n",
      "Epoch 8/1000\n",
      "22105/22105 [==============================] - 161s - loss: 0.1814 - f1: 0.9100 - val_loss: 0.2248 - val_f1: 0.8881\n",
      "Epoch 9/1000\n",
      "22105/22105 [==============================] - 162s - loss: 0.1760 - f1: 0.9137 - val_loss: 0.2257 - val_f1: 0.8922\n",
      "Epoch 10/1000\n",
      "22105/22105 [==============================] - 164s - loss: 0.1710 - f1: 0.9181 - val_loss: 0.2229 - val_f1: 0.8941\n",
      "Epoch 11/1000\n",
      "22105/22105 [==============================] - 159s - loss: 0.1665 - f1: 0.9199 - val_loss: 0.2252 - val_f1: 0.8948\n",
      "Epoch 12/1000\n",
      "22105/22105 [==============================] - 166s - loss: 0.1607 - f1: 0.9247 - val_loss: 0.2194 - val_f1: 0.8956\n",
      "Epoch 13/1000\n",
      "22105/22105 [==============================] - 154s - loss: 0.1561 - f1: 0.9262 - val_loss: 0.2201 - val_f1: 0.8971\n",
      "Epoch 14/1000\n",
      "22105/22105 [==============================] - 154s - loss: 0.1519 - f1: 0.9285 - val_loss: 0.2232 - val_f1: 0.8968\n",
      "Epoch 15/1000\n",
      "22105/22105 [==============================] - 146s - loss: 0.1475 - f1: 0.9318 - val_loss: 0.2195 - val_f1: 0.8965\n",
      "Epoch 16/1000\n",
      "22105/22105 [==============================] - 151s - loss: 0.1453 - f1: 0.9322 - val_loss: 0.2460 - val_f1: 0.8724\n",
      "Epoch 17/1000\n",
      "22105/22105 [==============================] - 151s - loss: 0.1441 - f1: 0.9342 - val_loss: 0.2228 - val_f1: 0.8974\n",
      "Epoch 18/1000\n",
      "22105/22105 [==============================] - 146s - loss: 0.1377 - f1: 0.9376 - val_loss: 0.2322 - val_f1: 0.8865\n",
      "Epoch 19/1000\n",
      "22105/22105 [==============================] - 147s - loss: 0.1326 - f1: 0.9393 - val_loss: 0.2242 - val_f1: 0.9001\n",
      "Epoch 20/1000\n",
      "22105/22105 [==============================] - 163s - loss: 0.1330 - f1: 0.9398 - val_loss: 0.2244 - val_f1: 0.9016\n",
      "Epoch 21/1000\n",
      "22105/22105 [==============================] - 158s - loss: 0.1285 - f1: 0.9418 - val_loss: 0.2283 - val_f1: 0.9001\n",
      "Epoch 22/1000\n",
      "22105/22105 [==============================] - 155s - loss: 0.1220 - f1: 0.9464 - val_loss: 0.2263 - val_f1: 0.9022\n",
      "Epoch 23/1000\n",
      "22105/22105 [==============================] - 155s - loss: 0.1187 - f1: 0.9469 - val_loss: 0.2385 - val_f1: 0.8975\n",
      "Epoch 24/1000\n",
      "22105/22105 [==============================] - 156s - loss: 0.1160 - f1: 0.9493 - val_loss: 0.2340 - val_f1: 0.8964\n",
      "Epoch 25/1000\n",
      "22105/22105 [==============================] - 158s - loss: 0.1155 - f1: 0.9496 - val_loss: 0.2349 - val_f1: 0.8911\n",
      "Epoch 26/1000\n",
      "22105/22105 [==============================] - 156s - loss: 0.1118 - f1: 0.9522 - val_loss: 0.2499 - val_f1: 0.8854\n",
      "Epoch 27/1000\n",
      "22105/22105 [==============================] - 166s - loss: 0.1072 - f1: 0.9542 - val_loss: 0.2410 - val_f1: 0.8969\n",
      "Epoch 28/1000\n",
      "22105/22105 [==============================] - 169s - loss: 0.1041 - f1: 0.9572 - val_loss: 0.2428 - val_f1: 0.8984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd806575ba8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "NAME = \"dotrenovana\"\n",
    "# input initialization\n",
    "in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "# Lambda layer for ohe transformation\n",
    "embedded = Lambda(ohe, output_shape=lambda x: (x[0], x[1], vocab_size), arguments={\"sz\": vocab_size})(in_sentence)\n",
    "block = embedded\n",
    "# convolutions with MaxPooling\n",
    "for i in range(3):\n",
    "    block = Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\", trainable = True)(block)\n",
    "    if i == 0:\n",
    "        block = MaxPooling1D(pool_size=5)(block)\n",
    "# LSTM cell\n",
    "block = LSTM(128, dropout=0.1, recurrent_dropout=0.1, trainable = True)(block)\n",
    "block = Dense(100, activation='relu', trainable = True)(block)\n",
    "block = Dense(1, activation='sigmoid', trainable = True)(block)\n",
    "\n",
    "# callbacks initialization\n",
    "# automatic generation of learning curves\n",
    "callback_1 = TensorBoard(log_dir='../logs/logs_{}'.format(NAME), histogram_freq=0,\n",
    "                             write_graph=False, write_images=False)\n",
    "# stop training model if accuracy does not increase more than five epochs\n",
    "callback_2 = EarlyStopping(monitor='val_f1', min_delta=0, patience=5, verbose=0, mode='max')\n",
    "# best model saving\n",
    "callback_3 = ModelCheckpoint(\"../models/model_{}.hdf5\".format(NAME), monitor='val_f1',\n",
    "                                 save_best_only=True, verbose=0, mode='max')\n",
    "\n",
    "# initialize model\n",
    "model = Model(inputs=in_sentence, outputs=block)\n",
    "model.load_weights('../models/model_char_cnn_ohe.hdf5')\n",
    "\n",
    "opt = optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = opt, \n",
    "              metrics=[f1])\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train, labels_train, validation_data=[X_val, labels_val],\n",
    "          batch_size=256, epochs=1000, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
