{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "A convolutional neural network is designed to identify indicative local predictors in a large structure, and to combine them to produce a fixed size vector representation of the structure, capturing the local aspects that are most informative for the prediction task at hand. I.e., the convolutional architecture will identify ngrams that are predictive for the task at hand, without the need to pre-specify an embedding vector for each possible ngram. The convolutional architecture also allows to share predictive behavior between ngrams that share similar components, even if the exact ngram was never seen at test time.\n",
    "\n",
    "The convolutional architecture could be expanded into a hierarchy of convolution layers,\n",
    "each one effectively looking at a longer range of ngrams in the sentence. This also allows the model to be sensitive to some non-contiguous ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# data path initialization\n",
    "BASE_DIR = '../'\n",
    "TEXT_DATA_DIR = BASE_DIR + 'data/'\n",
    "TEXT_DATA_FILE = \"movie_reviews.csv\"\n",
    "HEADER = True\n",
    "\n",
    "# parameters initialization\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # function for loading data\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(os.path.join(TEXT_DATA_DIR, TEXT_DATA_FILE), \"r\", encoding=\"utf-8\") as f:\n",
    "        if HEADER:\n",
    "            _ = next(f)\n",
    "        for line in f:\n",
    "            temp_y, temp_x = line.rstrip(\"\\n\").split(\",\", 1)\n",
    "            x.append(temp_x.replace(\"'\", \"\"))\n",
    "            y.append(temp_y)\n",
    "    return x, y\n",
    "\n",
    "data, labels = load_data()\n",
    "labels = np.asarray(labels, dtype='int8')\n",
    "\n",
    "# spliting our original data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val = \\\n",
    "    train_test_split(data, np.asarray(labels, dtype='int8'),\n",
    "                     test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=labels)\\\n",
    "    \n",
    "\n",
    "# initialize dictionary size and maximum sentence length\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "# create a dictionary with Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.!\"?`')\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "\n",
    "# replacing words with their indexes from our dictionary\n",
    "X_train = tokenizer.texts_to_sequences(data_train)\n",
    "X_val = tokenizer.texts_to_sequences(data_val)\n",
    "\n",
    "# fit each sentence to max length\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to embeddings file\n",
    "EMBEDDINGS_DIR = BASE_DIR + 'embeddings'\n",
    "EMBEDDINGS_FILE = 'glove.6B.50d.txt'\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# choose only 10000 words from our dictionary\n",
    "first_10000 = {k: v for k, v in tokenizer.word_index.items() if v < 10000}\n",
    "\n",
    "# upload embeddings\n",
    "embeddings = {}\n",
    "with zipfile.ZipFile(os.path.join(EMBEDDINGS_DIR, EMBEDDINGS_FILE+'.zip')) as myzip:\n",
    "    with myzip.open(EMBEDDINGS_FILE) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0].decode('UTF-8')\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        del values, word, coefs, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embeddings matrix where each row is word index\n",
    "\n",
    "embedding_matrix = np.zeros((tokenizer.num_words, EMBEDDING_DIM))\n",
    "for word, i in first_10000.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's create a simple convolutional neural network on pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 37, 100)           20100     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 34, 100)           40100     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 31, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 620,601\n",
      "Trainable params: 120,601\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "# инициализируем слой эмбеддингов\n",
    "NAME = \"words_cnn\"\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "# initialize model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is worse than on rnns. Train – **80.55%**, validation – **76.10**. \n",
    "\n",
    "## Task 1\n",
    "\n",
    "Try to improve this architecture and achieve better score.\n",
    "\n",
    "<details>\n",
    "  <summary>My solution</summary>\n",
    "    <pre>\n",
    "      <code>\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "      </code>\n",
    "    </pre>\n",
    "\n",
    "</details>\n",
    "\n",
    "***\n",
    "\n",
    "My new score is on train – **79.30%** and on validation – **77.88%**.\n",
    "\n",
    "About some tricks:\n",
    "\n",
    "### [Spacial Dropout](https://arxiv.org/pdf/1411.4280.pdf)\n",
    "\n",
    "This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, **spacial dropout** will help promote independence between feature maps and should be used instead.\n",
    "\n",
    "Here is an illustration of usual dropout:\n",
    "<img src=\"https://photos-6.dropbox.com/t/2/AAD9bnES_baGH4NCCu8qwAiynRYb9n9NcDZ--ys1sO-axw/12/533843084/png/32x32/1/_/1/2/dropout.png/EKyZ0aIEGN8iIAIoAg/MMO1Y426ocFL2W0DQE8thr6a1n-lGkpMtl6tW7Y0FuA?size=2048x1536&size_mode=3\" alt=\"dropout\" style=\"width: 700px;\"/>\n",
    "\n",
    "And spacial dropout:\n",
    "\n",
    "<img src=\"https://photos-2.dropbox.com/t/2/AAAwzLqr2EYT0Z3ehe_7NBz0AfqhCkOlBg6SrtdS1E7QYA/12/533843084/png/32x32/1/_/1/2/spacial_dropout.png/EKyZ0aIEGN8iIAIoAg/yvRhdcui-E_uvIMJHQ6WS_vFuD6i11Gc0BA9inTSqAQ?size=2048x1536&size_mode=3\" alt=\"dropout\" style=\"width: 700px;\"/>\n",
    "\n",
    "### [BatchNormalization](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "Consider a batch of activations at some layer. To make each dimension unit gaussian, apply:\n",
    "\n",
    "$$ \\Large \\widehat{x}^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}} $$ where the expectation and variance are computed over the training data set.\n",
    "\n",
    "But as we want to perform identity transformations, we should scale and shift our \"new\" batch. \n",
    "\n",
    "So our output will be: $$ \\Large y^{(k)} = \\gamma^{(k)}\\widehat{x}^{(k)} + \\beta^{(k)} $$\n",
    "\n",
    "Algorithm:\n",
    "1. Compute the empirical mean and variance independently for each dimension.\n",
    "2. Normalize\n",
    "\n",
    "Profit:\n",
    "- Improves gradient flow through the network\n",
    "- Allows higher learning rates\n",
    "- Reduces the strong dependence\n",
    "on initialization\n",
    "- Acts as a form of regularization\n",
    "in a funny way, and slightly reduces the need for dropout, maybe\n",
    "\n",
    "**Note: at test time BatchNorm layer functions differently:**\n",
    "The mean/std are not computed based on the batch. Instead, a single fixed empirical mean of activations during training is used.\n",
    "\n",
    "***\n",
    "\n",
    "Another approach could be not to use pretrained word vectors, but to train them with other weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 37, 100)           20100     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 34, 100)           40100     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 31, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 620,601\n",
      "Trainable params: 620,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            #weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test],\n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It trains faster (4sec on GPU) and accuracy is higher: train – **81.53%**, validation – **79.77%**. \n",
    "\n",
    "\n",
    "For some task in NLP it is good to use positional embedding. But on our task it doesn't increase accuracy for this task :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 40)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 40)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 40, 50)        500000      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 40, 20)        200000      input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 40, 70)        0           embedding_4[0][0]                \n",
      "                                                                   embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 37, 100)       28100       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalMa (None, 100)           0           conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 100)           10100       global_max_pooling1d_3[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             101         dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 738,301\n",
      "Trainable params: 238,301\n",
      "Non-trainable params: 500,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate, Input\n",
    "from keras.models import Model\n",
    "\n",
    "POS_EMB_SIZE = 20\n",
    "\n",
    "X_train_emb = np.array([list(range(MAX_SEQUENCE_LENGTH))]*len(X_train))\n",
    "X_val_emb = np.array([list(range(MAX_SEQUENCE_LENGTH))]*len(X_val))\n",
    "\n",
    "embedding_layer_pos = Embedding(tokenizer.num_words,\n",
    "                                POS_EMB_SIZE,\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "\n",
    "\n",
    "words_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "x1 = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)(words_input)\n",
    "pos_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "x2 = Embedding(tokenizer.num_words,\n",
    "                                POS_EMB_SIZE,\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)(pos_input)\n",
    "x = concatenate([x1, x2])\n",
    "\n",
    "x = Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\")(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=[words_input,pos_input], outputs=output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "#model.fit([X_train, X_train_emb], labels_train, validation_data=([X_val, X_val_emb], labels_val),\n",
    "#          batch_size=1024, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilated Convolutions\n",
    "\n",
    "In simple terms, dilated convolution is just a convolution applied to input with defined gaps. With this definitions, given our input is an 2D image, dilation rate k=1 is normal convolution and k=2 means skipping one pixel per input and k=3 means skipping 2 pixels.\n",
    "\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-d9025e88d7d792e26f4040b767b25819\" alt=\"dilated_conv\" style=\"width: 700px;\"/>\n",
    "\n",
    "In a dilated convolution architecture the hierarchy of convolution layers each has a stride size of k - 1. This allows an exponential growth in the effective window size as a function of the number of layers. With this purpose, it finds usage in applications cares more about integrating knowledge of the wider context with less cost.\n",
    "\n",
    "Let's compare:\n",
    "\n",
    "**Usual convolutions**\n",
    "<img src=\"https://photos-3.dropbox.com/t/2/AADAz0GQyUNqhNUJZZinmOkeHZIxheD_7IX4lCYytgUJ8Q/12/533843084/png/32x32/1/_/1/2/conv.png/EKyZ0aIEGOEiIAIoAg/k_P0Y9i1PEsQSbw2wWtPyd_WLhEEQ-WeU-RUL3vMWzw?size=2048x1536&size_mode=3\" alt=\"simple_conv\" style=\"width: 700px;\"/>\n",
    "\n",
    "**Dilated convolutions**\n",
    "<img src=\"https://photos-2.dropbox.com/t/2/AAA5X_b80s2Ex4xai2phgske3Noh9fyQe6zjXReaSQc1tg/12/533843084/png/32x32/1/_/1/2/dil_conv.png/EKyZ0aIEGOEiIAIoAg/q_iqx-TzLxuzE_vOPw77As1zM9rV6l8LOC5Ds7wCAKU?size=2048x1536&size_mode=3\" alt=\"simple_conv\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 31, 100)           20100     \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 22, 100)           40100     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 13, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 620,601\n",
      "Trainable params: 620,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=5, padding=\"valid\", dilation_rate=1))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=5, padding=\"valid\", dilation_rate=2))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test],\n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such architecture gives small increase in accuracy: train – **82.28%**, validation – **80.39%**.\n",
    "\n",
    "Let's remind that convolution neural network works as ngram model. In previous example we used only one kernal size. What if we try to create many separate convolutions with different kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 40)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 40, 50)        500000      input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)               (None, 40, 100)       10100       embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)               (None, 40, 100)       15100       embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)               (None, 40, 100)       20100       embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)               (None, 40, 100)       25100       embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 40, 400)       0           conv1d_35[0][0]                  \n",
      "                                                                   conv1d_36[0][0]                  \n",
      "                                                                   conv1d_37[0][0]                  \n",
      "                                                                   conv1d_38[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (GlobalM (None, 400)           0           concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_28 (Dense)                 (None, 100)           40100       global_max_pooling1d_11[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dense_29 (Dense)                 (None, 1)             101         dense_28[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 610,601\n",
      "Trainable params: 610,601\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate, Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "words_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "\n",
    "x = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)(words_input)\n",
    "\n",
    "x1 = Conv1D(activation=\"relu\", filters=100, kernel_size=2, padding=\"same\")(x)\n",
    "x2 = Conv1D(activation=\"relu\", filters=100, kernel_size=3, padding=\"same\")(x)\n",
    "x3 = Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"same\")(x)\n",
    "x4 = Conv1D(activation=\"relu\", filters=100, kernel_size=5, padding=\"same\")(x)\n",
    "x = concatenate([x1,x2,x3,x4])\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=words_input, outputs=output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "#model.fit([X_train, X_train_emb], labels_train, validation_data=([X_val, X_val_emb], labels_val),\n",
    "#          batch_size=1024, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As expected it was a good idea to do it. Train accuracy is **84.88%** and validation is **81.61%**.\n",
    "\n",
    "***\n",
    "\n",
    "## Neural networks on symbols\n",
    "\n",
    "There are two main types of preprocessing for neural networks on symbols:\n",
    "- similarly to words, train embeddings of symbols;\n",
    "- symbols represent as OHE embeddings.\n",
    "\n",
    "First, we'll figure out which characters to include. We can include any characters, but the general practice for English is to use 70 characters: lowercase letters, numbers and punctuation. Depending on the task or language, you can vary the number of characters.\n",
    "\n",
    "Let's start with a small task.\n",
    "\n",
    "## Task 2\n",
    "\n",
    "Create a dictionary of 70 characters and replace the symbols with their indexes. \n",
    "\n",
    "*Tip: use the `string` library and the` from keras.preprocessing.sequence import pad_sequences` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def create_vocab_set():\n",
    "    \n",
    "    #1. Your code\n",
    "    return vocab, vocab_size\n",
    "\n",
    "def text2sequence(text, vocab):\n",
    "    temp = []\n",
    "    #2. Your code\n",
    "    return temp\n",
    "\n",
    "vocab, vocab_size = create_vocab_set()\n",
    "\n",
    "X_train = text2sequence(data_train, vocab)\n",
    "X_test = text2sequence(data_test, vocab)\n",
    "\n",
    "#3. Your code\n",
    "X_train = \n",
    "X_val = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Here is a right answer!</summary>\n",
    "      <pre>\n",
    "            <code>\n",
    "              1. alphabet = (list(string.ascii_lowercase) + list(string.digits) + \n",
    "                              list(string.punctuation) + [' ', '\\n'])\n",
    "                 vocab_size = len(alphabet)\n",
    "                 vocab = {}\n",
    "                 for ix, t in enumerate(alphabet):\n",
    "                     vocab[t] = ix+1\n",
    "              2. for review in text:\n",
    "                     temp.append([])\n",
    "                     for i in review:\n",
    "                         char = vocab.get(i,0)\n",
    "                         if char != 0:\n",
    "                             temp[-1].append(char)\n",
    "              3. X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "                 X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "            </code>\n",
    "      </pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 40, 50)            3550      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 37, 100)           20100     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 34, 100)           40100     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 31, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 124,151\n",
      "Trainable params: 124,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GlobalMaxPooling1D\n",
    "NAME = \"char_cnn_emb\"\n",
    "EMBEDDING_DIM = 50\n",
    "vocab_size = 70\n",
    "\n",
    "# инициализируем модель\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test],\n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy is **82.21%**, validation – **78.31%**.\n",
    "\n",
    "The main drawback of this approach is that we do not take into account the order of words (nonlinear transformations are performed, and then the best features are selected and several fully connected layers are applied on them).\n",
    "\n",
    "Now consider the second approach to character models - ** One-Hot-Encoding (OHE) **. Suppose we have a dictionary of three characters \"a\", \"b\", \"c\". OHE representation of \"abca\" will be $$ a - 0 0 \\\\ b - 1 0 \\\\ c - 0 1 \\\\ а - 0 0 $$\n",
    "\n",
    "For implementation, we will use the additional functions from Api Keras and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# ohe функция\n",
    "def ohe(x, sz):\n",
    "    return tf.to_float(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 40, 70)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 37, 100)           28100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 7, 100)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 4, 100)            40100     \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 1, 100)            40100     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 238,549\n",
      "Trainable params: 238,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.layers import MaxPooling1D, LSTM\n",
    "\n",
    "NAME = \"char_cnn_ohe\"\n",
    "# инициализация входа\n",
    "in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "# Lambda слой для ohe преобразования\n",
    "embedded = Lambda(ohe, output_shape=lambda x: (x[0], x[1], vocab_size), arguments={\"sz\": vocab_size})(in_sentence)\n",
    "block = embedded\n",
    "# свертки с MaxPooling\n",
    "for i in range(3):\n",
    "    block = Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\")(block)\n",
    "    if i == 0:\n",
    "        block = MaxPooling1D(pool_size=5)(block)\n",
    "# LSTM ячейка\n",
    "block = LSTM(128, dropout=0.1, recurrent_dropout=0.1)(block)\n",
    "block = Dense(100, activation='relu')(block)\n",
    "block = Dense(1, activation='sigmoid')(block)\n",
    "# собираем модель\n",
    "model = Model(inputs=in_sentence, outputs=block)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test],\n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It slightly improves our previous model: train – **80.91%**, validation – **78.65%**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
